# EBS-Insight Performance Recommendations

## Current Performance Issues

**Problem**: Ollama summarization taking 60+ seconds for 381 row results
**Root Cause**: Large prompt size + context processing overhead

---

## âœ… Already Implemented (Today)

### 1. **Prompt Size Reduction** 
- System prompt: 620 chars â†’ 220 chars (65% reduction)
- Context prompt: Removed verbose metadata (version, intent, description)
- Data format: Markdown tables â†’ Compact key=value format
- Large datasets (>10 rows): Show only 3 samples + count
- **Impact**: ~70% prompt size reduction

### 2. **Ollama Parameter Tuning**
```python
"num_ctx": 1536,         # Reduced context window (was 2048)
"num_predict": 200,      # Max tokens (was 250)
"num_thread": 8,         # CPU parallelization
"repeat_penalty": 1.2,   # Avoid repetition
"top_k": 40,             # Limit sampling
"num_batch": 512,        # Batch processing
"keep_alive": "30m"      # Keep in RAM
```
**Impact**: ~40% faster inference

### 3. **Timeout Removed**
- `timeout_seconds: None` (was 60s)
- Let model complete without interruption
- **Impact**: No premature failures

---

## ðŸ”® Additional Recommendations

### **Short-Term (1-2 days)**

#### A) Model Quantization
**Current**: Likely using full precision model (FP16/FP32)
**Recommendation**: Use quantized model (Q4_K_M or Q5_K_M)
```bash
# Pull quantized version
ollama pull ebs-qwen25chat:q4_k_m
```
**Impact**: 2-4x faster inference, 50% less RAM
**Trade-off**: Minimal quality loss (<5%)

#### B) Parallel Query Execution
**Current**: Queries run sequentially
**Recommendation**: Run independent queries in parallel
```python
# In db/executor.py
from concurrent.futures import ThreadPoolExecutor

def execute_control_parallel(control):
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(execute_query, q) for q in control.queries]
        results = [f.result() for f in futures]
```
**Impact**: 30-50% faster DB execution for multi-query controls

#### C) Response Caching
**Current**: Every request hits Ollama
**Recommendation**: Cache identical prompts for 5 minutes
```python
# Simple in-memory cache
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def cached_summarize(prompt_hash, ...):
    return ollama_client.summarize(...)
```
**Impact**: Instant response for repeated queries

---

### **Medium-Term (1 week)**

#### D) Prompt Template Optimization
**Current**: Building prompts dynamically every time
**Recommendation**: Pre-compile control-specific templates
```python
# In controls/schema.py
class ControlDefinition:
    prompt_template: str = Field(...)  # Pre-optimized template per control
    
# For invalid_objects:
prompt_template = "Invalid objects: {count}. Types: {types}. Action: {action}"
```
**Impact**: 20-30% less processing overhead

#### E) Smart Sampling
**Current**: First N rows
**Recommendation**: Statistical sampling (spread across dataset)
```python
# Show rows at percentiles: 0%, 25%, 50%, 75%, 100%
def sample_distributed(rows, n=5):
    indices = [int(i * len(rows) / (n-1)) for i in range(n)]
    return [rows[i] for i in indices]
```
**Impact**: Better data representation with same prompt size

#### F) Ollama GPU Acceleration
**Current**: Likely CPU-only
**Check**:
```bash
ollama ps  # Check if using GPU
nvidia-smi  # If NVIDIA GPU available
```
**Recommendation**: Enable GPU if available
```bash
# Set environment variable
export OLLAMA_GPU=1
# Or in Dockerfile/systemd service
```
**Impact**: 5-10x faster inference (if GPU available)

---

### **Long-Term (1 month)**

#### G) Custom Fine-Tuned Model
**Current**: Generic Qwen model
**Recommendation**: Fine-tune on EBS-specific queries
- Collect 100+ query/response pairs
- Fine-tune smaller model (1.5B or 3B params)
- Optimize for speed on your specific tasks
**Impact**: 3-5x faster + better quality

#### H) Streaming Responses
**Current**: Wait for full response
**Recommendation**: Stream tokens to UI as generated
```python
# In client.py
response = requests.post(..., json={"stream": True})
for line in response.iter_lines():
    yield line  # Send to UI incrementally
```
**Impact**: Perceived latency reduction (users see progress)

#### I) Pre-compute Common Queries
**Current**: Real-time execution every time
**Recommendation**: Scheduled background jobs for common checks
```python
# Cron job: Every 5 minutes
# Pre-run: invalid_objects, concurrent_managers, workflow_status
# Store results in Redis/DB
# Serve from cache if <5min old
```
**Impact**: Sub-second response for common queries

---

## ðŸ“Š Expected Performance Improvements

| Optimization | Effort | Speed Gain | Priority |
|--------------|--------|------------|----------|
| Already Done | âœ… | ~60% | DONE |
| Quantized Model | 1h | 2-4x | **HIGH** |
| Response Caching | 2h | âˆž (for repeats) | **HIGH** |
| Parallel Queries | 4h | 30-50% | MEDIUM |
| GPU Acceleration | 1-2h | 5-10x | **HIGH** (if GPU) |
| Prompt Templates | 8h | 20-30% | MEDIUM |
| Smart Sampling | 4h | 15-20% | LOW |
| Fine-Tuned Model | 2-3 days | 3-5x | LOW |
| Streaming | 6h | UX only | MEDIUM |
| Pre-compute | 1 day | 95% | LOW |

---

## ðŸŽ¯ Recommended Next Steps

### Immediate (Today):
1. âœ… Prompt optimization (DONE)
2. âœ… Timeout removal (DONE)
3. âœ… Parameter tuning (DONE)

### Tomorrow:
4. **Switch to quantized model** (highest impact/effort ratio)
5. **Add response caching** (instant for repeats)

### Next Week:
6. Check GPU availability â†’ enable if possible
7. Implement parallel query execution
8. Add prompt templates per control

---

## ðŸ“ˆ Monitoring

Add these metrics to track improvements:
```python
# In observability/logger.py
metrics = {
    "ollama_prompt_size_bytes": len(prompt),
    "ollama_inference_time_ms": duration,
    "ollama_tokens_generated": token_count,
    "cache_hit_rate": cache_hits / total_requests,
    "db_parallel_speedup": sequential_time / parallel_time
}
```

---

## ðŸš¨ Critical Finding

**Current issue**: Even with optimizations, 60s timeout suggests:
1. Model too large for hardware (consider quantized)
2. CPU-only inference (check for GPU)
3. Ollama not optimized (check `ollama ps` for load status)

**Debug commands**:
```bash
# Check Ollama status
ollama ps

# Check model size
ollama show ebs-qwen25chat:latest

# Test inference speed
time ollama run ebs-qwen25chat:latest "test prompt"

# Check system resources
htop  # CPU usage
nvidia-smi  # GPU usage (if available)
```

**Expected performance** (with optimizations):
- Small prompts (<500 chars): 1-3s
- Medium prompts (500-2000 chars): 3-10s  
- Large prompts (2000+ chars): 10-30s

If still >30s â†’ hardware/model size mismatch.
